{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dim ordering\n",
    "print(k.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer seed for any randomness \n",
    "seed = np.int64(np.floor(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete all data in working directory made by below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! find ./data/final_sim_data/working_set/ -type f -name '*.png' -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS REALLY GOING TO REQUIRE SOME DOUBLE CHECKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_dir_for_keras(seed):\n",
    "    data_dir = os.getcwd() + '/data/final_sim_data'\n",
    "    full_data_dir = data_dir + '/full_set'\n",
    "    working_data_dir = data_dir + '/working_set'\n",
    "\n",
    "    for class_name in os.listdir(full_data_dir):\n",
    "        D_class = []\n",
    "    \n",
    "        for filename in glob.glob(full_data_dir + '/' + class_name + '/*.png'):\n",
    "            D_class.append(filename)\n",
    "\n",
    "        D_class = shuffle(D_class, random_state=seed)\n",
    "    \n",
    "        class_count = len(D_class)\n",
    "        train_val_index = int(np.floor(len(D_class) * 0.70))\n",
    "        val_test_index = int(np.floor(len(D_class) * 0.85))\n",
    "\n",
    "        for file in D_class[:train_val_index]:\n",
    "            shutil.copy2(file, \n",
    "                         working_data_dir + '/train/' + class_name + '/' + os.path.basename(os.path.normpath(file)))\n",
    "        for file in D_class[train_val_index:val_test_index]:\n",
    "            shutil.copy2(file, \n",
    "                         working_data_dir + '/val/' + class_name + '/' + os.path.basename(os.path.normpath(file)))\n",
    "        for file in D_class[val_test_index:]:\n",
    "            shutil.copy2(file, \n",
    "                         working_data_dir + '/test/' + class_name + '/' + os.path.basename(os.path.normpath(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_data_dir_for_keras(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper stolen \n",
    "def get_nb_files(directory):\n",
    "  \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  cnt = 0\n",
    "  for r, dirs, files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "  return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "In order to calculate the class weight do the following\n",
    "\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_dict = {\n",
    "#     'red': .75,\n",
    "#     'yellow': 4.5,\n",
    "#     'green': 4,\n",
    "#     'nolight': 4\n",
    "# }\n",
    "\n",
    "class_list = [\n",
    "    'nolight',\n",
    "    'green',\n",
    "    'yellow',\n",
    "    'red' \n",
    "]\n",
    "\n",
    "weight_dict = {\n",
    "    0: 4,\n",
    "    1: 4,\n",
    "    2: 4.5,\n",
    "    3: .75,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 299, 299\n",
    "\n",
    "TRAIN_DATA_DIR = \"./data/final_sim_data/working_set/train\"\n",
    "VAL_DATA_DIR = \"./data/final_sim_data/working_set/val\"\n",
    "TEST_DATA_DIR = \"./data/final_sim_data/working_set/test\"\n",
    "\n",
    "NB_TRAIN_SAMPLES = get_nb_files(TRAIN_DATA_DIR)\n",
    "NB_VAL_SAMPLES = get_nb_files(VAL_DATA_DIR)\n",
    "NB_TEST_SAMPLES = get_nb_files(TEST_DATA_DIR)\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "EPOCHS_TRANSFER = 5\n",
    "EPOCHS_FINE_TUNE = 15\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH = int(np.ceil(NB_TRAIN_SAMPLES / BATCH_SIZE))\n",
    "VAL_STEPS_PER_EPOCH = int(np.ceil(NB_VAL_SAMPLES / BATCH_SIZE))\n",
    "TEST_STEPS_PER_EPOCH = int(np.ceil(NB_TEST_SAMPLES / BATCH_SIZE))\n",
    "\n",
    "FROZEN_LAYERS = 172"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MODIFY COLOR, REMOVING YELLOW FROM PICTURES\n",
    "# TODO: DO I NEED ANY ADDITIONAL MEAN NORMALIZTION?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12361 images belonging to 4 classes.\n",
      "Found 2649 images belonging to 4 classes.\n",
      "Found 2651 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range=0.4,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.5,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range=0.4,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.5,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "#     width_shift_range = 0.2,\n",
    "#     height_shift_range=0.4,\n",
    "#     rotation_range=20\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.5,\n",
    "#     fill_mode='nearest',\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    classes=class_list)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DATA_DIR,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    classes=class_list)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    classes=class_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "-rescale and fill dont seem to impact much\n",
    "-neither does class mode or class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "309/310 [============================>.] - ETA: 0s - loss: 1.7536 - acc: 0.5495Epoch 00000: val_loss improved from inf to 1.21675, saving model to inceptionv3.h5\n",
      "310/310 [==============================] - 390s - loss: 1.7635 - acc: 0.5477 - val_loss: 1.2168 - val_acc: 0.3356\n",
      "Epoch 2/2\n",
      "309/310 [============================>.] - ETA: 0s - loss: 1.4937 - acc: 0.6326Epoch 00001: val_loss improved from 1.21675 to 0.84243, saving model to inceptionv3.h5\n",
      "310/310 [==============================] - 370s - loss: 1.4912 - acc: 0.6306 - val_loss: 0.8424 - val_acc: 0.6006\n",
      "Epoch 1/2\n",
      "309/310 [============================>.] - ETA: 1s - loss: 1.0381 - acc: 0.7557Epoch 00000: val_loss improved from 0.84243 to 0.44583, saving model to inceptionv3.h5\n",
      "310/310 [==============================] - 388s - loss: 1.0356 - acc: 0.7565 - val_loss: 0.4458 - val_acc: 0.8230\n",
      "Epoch 2/2\n",
      "309/310 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.8179Epoch 00001: val_loss improved from 0.44583 to 0.37170, saving model to inceptionv3.h5\n",
      "310/310 [==============================] - 375s - loss: 0.7790 - acc: 0.8153 - val_loss: 0.3717 - val_acc: 0.8501\n",
      "['loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18173385175701717, 0.94530365335316269]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# + global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# + FC layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# + log laye\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# freeze all convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (most places, including the inceptv3 paper recommended RMSProp, \n",
    "#  but I had a lot of premature convergence issues with it. to be fair, I switched to\n",
    "#  adam experimenting around before I spent anytime optimizing LR, so who knows. converges\n",
    "#  quick enough for Anand's data as it is)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"inceptionv3.h5\", monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=3)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "# training for transfer learning\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs = EPOCHS_TRANSFER,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "    callbacks = [early, checkpoint],\n",
    "    verbose=True, \n",
    "    class_weight=weight_dict,\n",
    "    use_multiprocessing=True) #    workers=4\n",
    "\n",
    "# start fine-tuning conv layers in addition to FC. \n",
    "# freeze the bottom FROZEN_LAYERS layers, train the rest\n",
    "for layer in model.layers[:FROZEN_LAYERS]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[FROZEN_LAYERS:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# training for fine-tuning conv layers\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs = EPOCHS_FINE_TUNE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "    callbacks = [early, checkpoint], #\n",
    "    verbose=True,\n",
    "    class_weight=weight_dict,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "\n",
    "print(model.metrics_names)\n",
    "model.evaluate_generator(\n",
    "    test_generator,\n",
    "    steps = TEST_STEPS_PER_EPOCH,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'class_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-77e3d5fa69c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'class_indices'"
     ]
    }
   ],
   "source": [
    "model.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18173384965477868, 0.94530365784992954]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate_generator(\n",
    "    test_generator,\n",
    "    steps = TEST_STEPS_PER_EPOCH,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bunch of junk code for pushing around earlier data that I'm not sure I won't need again...\n",
    "\n",
    "# find . -type f -name '*.png' -delete\n",
    "\n",
    "# !! ls data/TrafficLightDataset/working_data/\n",
    "\n",
    "# with open('./data/TrafficLightDataset/labels_num.csv') as labels:\n",
    "#     c_labels = OrderedDict(csv.reader(labels))\n",
    "\n",
    "# X_labels = np.array(list(c_labels.keys()))\n",
    "# Y = np.array(list(c_labels.values()))\n",
    "# shuffle(X_labels, Y)\n",
    "\n",
    "# images = []\n",
    "# for label in X_labels:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/Capstone/ros/src/tl_detector/data/TrafficLightDataset/image_data/\" + label + \".png\"\n",
    "#     image = cv2.imread(loc, 1)\n",
    "# #     image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) \n",
    "#     # May not need to do resizing... BUT SHOULD VIEW IMAGES\n",
    "# #     resized_image = cv2.resize(image, (256, 256)) \n",
    "#     images.append(image)\n",
    "# X = np.array(images)\n",
    "# print(X.shape)\n",
    "\n",
    "# train_val_index = int(np.floor(len(Y) * 0.85))\n",
    "\n",
    "# D = list(zip(X, Y))\n",
    "# D_train = D[:train_val_index]\n",
    "# D_val = D[train_val_index:]\n",
    "# X_train, Y_train = zip(*D_train)\n",
    "# print(len(X_train))\n",
    "# X_val, Y_val = zip(*D_val)\n",
    "# print(len(X_val))\n",
    "\n",
    "# images = []\n",
    "# for x, y in D_train:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/\" + \\\n",
    "#           \"Capstone/ros/src/tl_detector/data/TrafficLightDataset/working_data/train/\" + \\\n",
    "#           str(y) + \"/\" + str(np.random.randint(100000000, 999999999)) + \".png\"\n",
    "#     cv2.imwrite(loc, x)\n",
    "    \n",
    "# for x, y in D_val:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/\" + \\\n",
    "#           \"Capstone/ros/src/tl_detector/data/TrafficLightDataset/working_data/val/\" + \\\n",
    "#           str(y) + \"/\" + str(np.random.randint(100000000, 999999999)) + \".png\"\n",
    "#     cv2.imwrite(loc, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
