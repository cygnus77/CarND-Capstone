{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dim ordering\n",
    "print(k.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bunch of junk code for pushing around earlier data that I'm not sure I won't need again...\n",
    "\n",
    "# find . -type f -name '*.png' -delete\n",
    "\n",
    "# !! ls data/TrafficLightDataset/working_data/\n",
    "\n",
    "# with open('./data/TrafficLightDataset/labels_num.csv') as labels:\n",
    "#     c_labels = OrderedDict(csv.reader(labels))\n",
    "\n",
    "# X_labels = np.array(list(c_labels.keys()))\n",
    "# Y = np.array(list(c_labels.values()))\n",
    "# shuffle(X_labels, Y)\n",
    "\n",
    "# images = []\n",
    "# for label in X_labels:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/Capstone/ros/src/tl_detector/data/TrafficLightDataset/image_data/\" + label + \".png\"\n",
    "#     image = cv2.imread(loc, 1)\n",
    "# #     image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) \n",
    "#     # May not need to do resizing... BUT SHOULD VIEW IMAGES\n",
    "# #     resized_image = cv2.resize(image, (256, 256)) \n",
    "#     images.append(image)\n",
    "# X = np.array(images)\n",
    "# print(X.shape)\n",
    "\n",
    "# train_val_index = int(np.floor(len(Y) * 0.85))\n",
    "\n",
    "# D = list(zip(X, Y))\n",
    "# D_train = D[:train_val_index]\n",
    "# D_val = D[train_val_index:]\n",
    "# X_train, Y_train = zip(*D_train)\n",
    "# print(len(X_train))\n",
    "# X_val, Y_val = zip(*D_val)\n",
    "# print(len(X_val))\n",
    "\n",
    "# images = []\n",
    "# for x, y in D_train:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/\" + \\\n",
    "#           \"Capstone/ros/src/tl_detector/data/TrafficLightDataset/working_data/train/\" + \\\n",
    "#           str(y) + \"/\" + str(np.random.randint(100000000, 999999999)) + \".png\"\n",
    "#     cv2.imwrite(loc, x)\n",
    "    \n",
    "# for x, y in D_val:\n",
    "#     loc = \"/home/dieslow/WORKSPACE/PROJECTS/CARZ/nanodegree/\" + \\\n",
    "#           \"Capstone/ros/src/tl_detector/data/TrafficLightDataset/working_data/val/\" + \\\n",
    "#           str(y) + \"/\" + str(np.random.randint(100000000, 999999999)) + \".png\"\n",
    "#     cv2.imwrite(loc, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper stolen \n",
    "def get_nb_files(directory):\n",
    "  \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  cnt = 0\n",
    "  for r, dirs, files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "  return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 299, 299\n",
    "\n",
    "# TRAIN_DATA_DIR = \"data/TrafficLightDataset/working_data/train\"\n",
    "# VAL_DATA_DIR = \"data/TrafficLightDataset/working_data/val\"\n",
    "TRAIN_DATA_DIR = \"./data/TrafficLightDataset/big_working_data/train\"\n",
    "VAL_DATA_DIR = \"./data/TrafficLightDataset/big_working_data/val\"\n",
    "TEST_DATA_DIR = \"./data/TrafficLightDataset/big_working_data/test\"\n",
    "\n",
    "NB_TRAIN_SAMPLES = get_nb_files(TRAIN_DATA_DIR)\n",
    "NB_VAL_SAMPLES = get_nb_files(VAL_DATA_DIR)\n",
    "NB_TEST_SAMPLES = get_nb_files(TEST_DATA_DIR)\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "EPOCHS_TRANSFER = 5\n",
    "EPOCHS_FINE_TUNE = 5\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH = int(np.ceil(NB_TRAIN_SAMPLES / BATCH_SIZE))\n",
    "VAL_STEPS_PER_EPOCH = int(np.ceil(NB_VAL_SAMPLES / BATCH_SIZE))\n",
    "TEST_STEPS_PER_EPOCH = int(np.ceil(NB_TEST_SAMPLES / BATCH_SIZE))\n",
    "\n",
    "FROZEN_LAYERS = 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12319 images belonging to 4 classes.\n",
      "Found 2728 images belonging to 4 classes.\n",
      "Found 2728 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "#     rescale = 1./255,\n",
    "#     horizontal_flip = True,\n",
    "#     zoom_range = 0.4,\n",
    "#     width_shift_range = 0.3,\n",
    "#     height_shift_range=0.3,\n",
    "#     rotation_range=20\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "#     horizontal_flip = True,\n",
    "#     rescale = 1./255,\n",
    "#     fill_mode = \"nearest\"\n",
    "#     zoom_range = 0.3,\n",
    "#     width_shift_range = 0.3,\n",
    "#     height_shift_range=0.3,\n",
    "#     rotation_range=30\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size = BATCH_SIZE)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DATA_DIR,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DATA_DIR,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "-rescale and fill dont seem to impact much\n",
    "-neither does class mode or class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12319 images belonging to 4 classes.\n",
      "Found 2728 images belonging to 4 classes.\n",
      "Found 2614 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8688Epoch 00000: val_loss improved from inf to 0.23132, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 262s - loss: 0.3946 - acc: 0.8688 - val_loss: 0.2313 - val_acc: 0.9183\n",
      "Epoch 2/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9244Epoch 00001: val_loss improved from 0.23132 to 0.23077, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 204s - loss: 0.2179 - acc: 0.9243 - val_loss: 0.2308 - val_acc: 0.9274\n",
      "Epoch 3/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9365Epoch 00002: val_loss improved from 0.23077 to 0.10981, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 222s - loss: 0.1785 - acc: 0.9364 - val_loss: 0.1098 - val_acc: 0.9666\n",
      "Epoch 4/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9451Epoch 00003: val_loss did not improve\n",
      "308/308 [==============================] - 229s - loss: 0.1546 - acc: 0.9450 - val_loss: 0.1134 - val_acc: 0.9644\n",
      "Epoch 5/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9426Epoch 00004: val_loss improved from 0.10981 to 0.10259, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 232s - loss: 0.1518 - acc: 0.9426 - val_loss: 0.1026 - val_acc: 0.9637\n",
      "Epoch 1/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9784Epoch 00000: val_loss improved from 0.10259 to 0.04355, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 253s - loss: 0.0684 - acc: 0.9785 - val_loss: 0.0436 - val_acc: 0.9835\n",
      "Epoch 2/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9918Epoch 00001: val_loss improved from 0.04355 to 0.03016, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 251s - loss: 0.0291 - acc: 0.9918 - val_loss: 0.0302 - val_acc: 0.9905\n",
      "Epoch 3/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9948Epoch 00002: val_loss improved from 0.03016 to 0.02527, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 253s - loss: 0.0186 - acc: 0.9948 - val_loss: 0.0253 - val_acc: 0.9927\n",
      "Epoch 4/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9967Epoch 00003: val_loss improved from 0.02527 to 0.02232, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 240s - loss: 0.0138 - acc: 0.9968 - val_loss: 0.0223 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "307/308 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9976Epoch 00004: val_loss improved from 0.02232 to 0.02078, saving model to inceptionv3.h5\n",
      "308/308 [==============================] - 249s - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0208 - val_acc: 0.9938\n",
      "['loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0358809883027198, 0.77505740999901851]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# + global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# + FC layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# + log laye\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# freeze all convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (most places, including the inceptv3 paper recommended RMSProp, \n",
    "#  but I had a lot of premature convergence issues with it. to be fair, I switched to\n",
    "#  adam experimenting around before I spent anytime optimizing LR, so who knows. converges\n",
    "#  quick enough for Anand's data as it is)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"inceptionv3.h5\", monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "# training for transfer learning\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs = EPOCHS_TRANSFER,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "    callbacks = [early, checkpoint],\n",
    "    verbose=True, \n",
    "    class_weight='auto',\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "# start fine-tuning conv layers in addition to FC. \n",
    "# freeze the bottom FROZEN_LAYERS layers, train the rest\n",
    "for layer in model.layers[:FROZEN_LAYERS]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[FROZEN_LAYERS:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# training for fine-tuning conv layers\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs = EPOCHS_FINE_TUNE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "    callbacks = [early, checkpoint], #\n",
    "    verbose=True,\n",
    "    class_weight='auto',\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "\n",
    "print(model.metrics_names)\n",
    "model.evaluate_generator(\n",
    "    test_generator,\n",
    "    steps = TEST_STEPS_PER_EPOCH,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
